{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Random generations of intensity measure values and failure probabilities through MC simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/29885371/how-do-i-block-comment-in-jupyter-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_values = 5 #for tests\n",
    "no_values = 512 #this should come from a convergence study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intentisity measure values for PGA which belongs to 0-1g range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_measure_min = 0 #value derived from the paper\n",
    "intensity_measure_max = 1.0 #value derived from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intensity measure values for scour depth which belongs to 0-max scour depth range, with max scour depth > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A transformation of standard uniform distribution is needed in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crack_starting_scour_depth = 1.00 #m\n",
    "collapse_scour_depth = 2.54 #m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0) #this numpy function ensures that every time the code is ran, the same random numbers are obtained. Therefore, it must be commented out to produce data for a probability paper.\n",
    "#https://www.geeksforgeeks.org/generate-random-numbers-from-the-uniform-distribution-using-numpy/#:~:text=To%20generate%20random%20numbers%20from,()%20method%20of%20random%20module.&text=In%20uniform%20distribution%20samples%20are,low%20but%20excludes%20high%20interval.\n",
    "#https://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do\n",
    "#https://www.tutorialspoint.com/generating-random-number-list-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(512, 1)\n",
      "[0.00027652 0.00189005 0.00293317 0.00441351 0.0044497 ]\n"
     ]
    }
   ],
   "source": [
    "intensity_measure_values = np.sort(np.random.uniform(size = no_values)) #the sort command is needed otherwise the uniform random numbers don't have an increasing order\n",
    "#https://sparkbyexamples.com/numpy/numpy-sort-arrays-examples/#:~:text=Array%20sort()-,Use%20numpy.,containing%20elements%20in%20ascending%20order.\n",
    "#intensity_measure_values = np.random.uniform(size = no_values)) #it gives a vector of random numbers, uniformly distributed, greater or smaller than the following one. -WRONG-\n",
    "\n",
    "intensity_measure_values_exp = np.expand_dims(intensity_measure_values, 1)\n",
    "\n",
    "print(type(intensity_measure_values))\n",
    "print(intensity_measure_values_exp.shape)\n",
    "print(intensity_measure_values[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1)\n"
     ]
    }
   ],
   "source": [
    "transformed_intensity_measure_values = (collapse_scour_depth - crack_starting_scour_depth) * intensity_measure_values + crack_starting_scour_depth\n",
    "\n",
    "transformed_intensity_measure_values_reshape = np.reshape(transformed_intensity_measure_values, (no_values,1))\n",
    "\n",
    "print(transformed_intensity_measure_values_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(512, 1)\n",
      "[[0.0036941 ]\n",
      " [0.00618508]\n",
      " [0.01080154]\n",
      " [0.01162867]\n",
      " [0.01243106]]\n"
     ]
    }
   ],
   "source": [
    "failure_probability_values = np.sort(np.random.uniform(size=no_values))\n",
    "#https://sparkbyexamples.com/numpy/numpy-sort-arrays-examples/#:~:text=Array%20sort()-,Use%20numpy.,containing%20elements%20in%20ascending%20order.\n",
    "failure_probability_values_exp = np.expand_dims(failure_probability_values, 1)\n",
    "\n",
    "print(type(intensity_measure_values))\n",
    "print(failure_probability_values_exp.shape)\n",
    "print(failure_probability_values_exp[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type([failure_probability_values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the fragility curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data needed are median and std of the fragility curve from the paper.\n",
    "#These values are referred as deterministic values.\n",
    "median_fragility = 1.215 #back-calculated from reverting the standard normal distribution, once the logtransformation was applied. It comes from the file median_std_deterministic.ipynb\n",
    "std_fragility = 0.05 #assumed as the value for the system-level fragility, because the paper does not specify a value. This value derives from fitting bridge damage data. Bridge damage data are obtained from numerical simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognorm_cdf(x, median, sigma):\n",
    "    shape = sigma #described by s in the scipy guide\n",
    "    loc = 0\n",
    "    scale = median\n",
    "    return stats.lognorm.cdf(x, shape, loc, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1)\n",
      "[[5.08779026e-05]\n",
      " [6.23359956e-05]\n",
      " [7.09756708e-05]\n",
      " [8.51588576e-05]\n",
      " [8.55364308e-05]]\n"
     ]
    }
   ],
   "source": [
    "# lognorm_values = lognorm_cdf(intensity_measure_values_exp, median_fragility, std_fragility) #for the PGA case\n",
    "lognorm_values = lognorm_cdf(transformed_intensity_measure_values, median_fragility, std_fragility) #for the scour depth case\n",
    "\n",
    "lognorm_values_reshape = np.reshape(lognorm_values, (no_values,1))\n",
    "\n",
    "print(lognorm_values_reshape.shape)\n",
    "print(lognorm_values_reshape[0:5])\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)}) #https://stackoverflow.com/questions/22222818/how-to-print-numpy-array-with-3-decimal-places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_condition=[]\n",
    "damage_condition = lognorm_values_reshape >= failure_probability_values_exp\n",
    "#https://thispointer.com/compare-two-numpy-arrays-element-wise-in-python/#compare-two-numpy-arrays-using-operator\n",
    "#https://www.w3schools.com/python/python_operators.asp\n",
    "#print(damage_condition.shape)\n",
    "#print(np.column_stack((lognorm_values_exp[0:5], failure_probability_values_exp[0:5])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#https://flexiple.com/python/typeerror-only-size-1-array-can-be-converted-to-python-scalars/\n",
    "damage_condition_boolean = damage_condition.astype(int)\n",
    "print(damage_condition_boolean[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.004 0.000 0.000]\n",
      " [0.006 0.000 0.000]\n",
      " [0.011 0.000 0.000]\n",
      " [0.012 0.000 0.000]\n",
      " [0.012 0.000 0.000]]\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/60493932/how-to-combine-column-vectors-into-a-matrix\n",
    "damage_probability_data = np.concatenate([failure_probability_values_exp, lognorm_values_reshape, damage_condition_boolean], axis=1)\n",
    "\n",
    "print(damage_probability_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Realisations of fragility paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai = intensity_measure_values_exp #for the PGA case\n",
    "ai = transformed_intensity_measure_values_reshape #for the scour depth case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ri = damage_condition_boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median = 0.5 #test value\n",
    "# sigma = 0.1 #test value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_logNorm(parameters):\n",
    "    median, sigma = parameters\n",
    "\n",
    "    Fi = lognorm_cdf(ai, median, sigma)\n",
    "    Li = np.power(Fi, ri) * np.power(1-Fi, 1-ri)\n",
    "\n",
    "    from math import nan, isnan\n",
    "    list_with_nan = Li\n",
    "    list_without_nan = [x for x in list_with_nan if isnan(x)==False]\n",
    "    \n",
    "    L_two = np.array(list_without_nan)\n",
    "    L_three = (L_two[L_two>0])\n",
    "    #log_L = np.log10(L)\n",
    "    log_L = np.log10(L_three)\n",
    "\n",
    "    LL = np.sum(log_L)\n",
    "    neg_LL = -1*LL\n",
    "\n",
    "    return neg_LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/57229402/how-to-restrict-optimized-value-to-be-greater-than-0\n",
    "#https://stackoverflow.com/questions/19244527/scipy-optimize-how-to-restrict-argument-values\n",
    "con = [{\"type\": \"ineq\", \"fun\": MLE_logNorm}]\n",
    "\n",
    "#Without specifying bands, negative values of standard deviation were obtained\n",
    "#The lower bounds assure that the obtained values are greater than zero, while the first upper bound are due to the values of the intensity measure.\n",
    "bnds = ((0.01, inf), (0.01, inf)) #without specifying bands, negative values of standard deviation were obtained\n",
    "\n",
    "#Initial guess value\n",
    "guess = np.array([1.0, 0.1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:569: RuntimeWarning: Method Nelder-Mead cannot handle constraints.\n",
      "  warn('Method %s cannot handle constraints.' % method,\n"
     ]
    }
   ],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n",
    "#https://stackoverflow.com/questions/30902111/passing-list-of-arguments-of-varying-length-to-scipy-optimize-minimize-understa\n",
    "#https://stackoverflow.com/questions/52670012/convergencewarning-liblinear-failed-to-converge-increase-the-number-of-iterati\n",
    "\n",
    "mle_model = minimize(MLE_logNorm, guess, method = 'Nelder-Mead', constraints = con, bounds = bnds, options = {'maxiter':5000}) #assumed no. of iterations\n",
    "\n",
    "#mle_model = minimize(MLE_logNorm, guess, method = 'Nelder-Mead', bounds = bnds, options = {'maxiter':5000}) #assumed no. of iterations\n",
    "\n",
    "# mle_model = minimize(MLE_logNorm, guess, method = 'L-BFGS-B', constraints = con, bounds = bnds, options = {'maxiter':5000}) #assumed no. of iterations\n",
    "\n",
    "#Different results are obtained by applying different minimization methods. \n",
    "#[0.99, 0.027] with Nelder-Mead, while [0.99, 0.010] with L-BFGS-B. \n",
    "# The listed results sound suspicious because equal to the set bounds. \n",
    "#Both the methods cannot handle the specified constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[1.135, 0.010],\n",
       "       [1.136, 0.010],\n",
       "       [1.136, 0.010]]), array([0.925, 0.925, 0.925]))\n",
       "           fun: 0.9251637899778014\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 51\n",
       "           nit: 28\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([1.135, 0.010])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate failure probability with realization values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000]\n",
      " [0.000]\n",
      " [0.000]\n",
      " [0.000]\n",
      " [0.000]]\n"
     ]
    }
   ],
   "source": [
    "Fi = lognorm_cdf(ai, mle_model.x[0], mle_model.x[1])\n",
    "print(Fi[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000],\n",
       "       [0.000],\n",
       "       [0.000],\n",
       "       [0.000],\n",
       "       [0.000]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.lognorm.cdf(ai, mle_model.x[1], loc=0, scale=mle_model.x[0])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigated_scour_depth = [1.00, 1.20, 1.30, 1.50, 2.00, 2.30, 2.49, 2.54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000]\n"
     ]
    }
   ],
   "source": [
    "Fi = lognorm_cdf(investigated_scour_depth, mle_model.x[0], mle_model.x[1])\n",
    "print(Fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
